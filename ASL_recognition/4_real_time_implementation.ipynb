{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5795819e-1ee6-4a92-8fff-53eecfbd9675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load your trained model\n",
    "model = load_model('asl_sign_language_model.h5')  \n",
    "labels = ['A', 'B', 'C', 'D', 'del', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'nothing', 'O', 'P', 'Q', 'R', 'S', 'space', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3e42ed2-8c9d-4cc3-b570-be28684039cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "IMG_SIZE = 64  # Image size for model input\n",
    "x, y, w, h = 300, 100, 300, 300  # Region of Interest (ROI) for hand signs\n",
    "\n",
    "# Function to preprocess the image before passing it to the model\n",
    "def preprocess_image(image):\n",
    "    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))  # Resize to model input size\n",
    "    image = image.astype('float32') / 255.0  # Normalize pixel values\n",
    "    return np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "\n",
    "# Start webcam and prediction loop\n",
    "cap = cv2.VideoCapture(0)  # Open the webcam\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    roi = frame[y:y+h, x:x+w]  # Extract the region of interest (hand sign)\n",
    "\n",
    "    # Process the image for model prediction\n",
    "    processed = preprocess_image(roi)\n",
    "    prediction = model.predict(processed, verbose=0)\n",
    "    pred_index = np.argmax(prediction)  # Get the predicted class index\n",
    "    pred_letter = labels[pred_index]  # Get the label for the predicted class\n",
    "\n",
    "    # Draw a rectangle around the hand and display the prediction\n",
    "    cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "    cv2.putText(frame, f\"Prediction: {pred_letter}\", (50, 80), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 3)\n",
    "\n",
    "    # Display the video with the predicted label\n",
    "    cv2.imshow(\"ASL Recognition\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to quit\n",
    "        break\n",
    "\n",
    "cap.release()  # Release the webcam\n",
    "cv2.destroyAllWindows()  # Close any open OpenCV windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e687da9-0e2c-47d2-a741-b237f50cffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for _ in range(5):  # Collect 5 frames before predicting\n",
    "    prediction = model.predict(processed, verbose=0)\n",
    "    predictions.append(np.argmax(prediction))\n",
    "\n",
    "final_prediction = np.bincount(predictions).argmax()  # Majority voting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6557223a-1e8a-495e-bbcf-02e189a09ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ADMIN\\AppData\\Local\\Temp\\tmpysi32s4l\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ADMIN\\AppData\\Local\\Temp\\tmpysi32s4l\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\ADMIN\\AppData\\Local\\Temp\\tmpysi32s4l'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 64, 64, 3), dtype=tf.float32, name='input_layer_2')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 29), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1679358769872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1679358770640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1679358770832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1679360722512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1679360724240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1679360724816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1679360725008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1679360725968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1679360723472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1679360727120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1679360727312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1679360728080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1679360728272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1679360729040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model\n",
    "with open('asl_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceabae24-4290-4146-bbad-6ef6632751ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
